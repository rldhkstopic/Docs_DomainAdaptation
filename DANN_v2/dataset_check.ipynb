{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(trainloader.dataset.data.shape) # 60000 * 28  28\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape[-1])\n",
    "images = torch.cat((images, images, images), 1)\n",
    "print(images.shape)\n",
    "# print(\"images shape:\", images.shape, \"labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18()\n",
    "print(model)\n",
    "\n",
    "# Input shape\n",
    "input_shape = (3, 224, 224)\n",
    "x = torch.randn(1, *input_shape)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Summary\n",
    "from torchsummary import summary\n",
    "summary(model, input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Only-Source Domain Test Start\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m net \u001b[39m=\u001b[39m Network(train_source, train_target)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# net._train_source(epochs=20)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# net._train_dann(epochs=100)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m net\u001b[39m.\u001b[39m_test_source()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(torch.cuda.device_count())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/_nex1/docs_DomainAdaptation/DANN_v2/domain.py:86\u001b[0m, in \u001b[0;36mNetwork._test_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_source:\n\u001b[1;32m     84\u001b[0m     images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), labels\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 86\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(images)\n\u001b[1;32m     87\u001b[0m     _, predicted \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n\u001b[1;32m     89\u001b[0m     total_source \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LIG/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIG/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:157\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n\u001b[0;32m--> 157\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule must have its parameters and buffers \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mon device \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (device_ids[0]) but found one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mthem on device: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj, t\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    161\u001b[0m inputs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscatter(inputs, kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids)\n\u001b[1;32m    162\u001b[0m \u001b[39m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu"
     ]
    }
   ],
   "source": [
    "from domain import Network\n",
    "from dataset import mnist_train_loader, mnistm_train_loader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "train_source, train_target = mnist_train_loader, mnistm_train_loader\n",
    "\n",
    "net = Network(train_source, train_target)\n",
    "# net._train_source(epochs=20)\n",
    "# net._train_dann(epochs=100)\n",
    "net._test_source()\n",
    "\n",
    "# print(torch.cuda.device_count())\n",
    "if torch.cuda.device_count() > 1:\n",
    "    layout = list(net.model.module.children())\n",
    "else:\n",
    "    layout = list(net.model.children())\n",
    "\n",
    "model = layout[:-1]\n",
    "model = nn.Sequential(*model)\n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    dummpy_input = torch.randn(1, 3, 28, 28).to(net.device)\n",
    "    output = model(dummpy_input)\n",
    "    print(output.shape)\n",
    "\n",
    "\n",
    "# print(net.E)\n",
    "# print(nn.Sequential(*list(net.model.children())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.model)\n",
    "\"\"\"\n",
    "ResNet18은 일반적으로 224x224x3 크기의 이미지를 입력을 처리하도록 설계되어있음\n",
    "하지만, 우리의 데이터셋은 28x28x3 크기의 이미지를 가짐\n",
    "따라서, ResNet18의 입력 크기를 조정해야 함\n",
    "\n",
    "ResNet18의 입력 크기를 조정하는 방법은 두 가지가 있음\n",
    "1. 입력 크기를 224x224x3으로 조정\n",
    "2. ResNet18의 첫 번째 계층을 3x3 커널, 2 스트라이드, 1 패딩으로 변경\n",
    "\n",
    "입력: 28x28x3\n",
    "\n",
    "첫 번째 Conv2d (7x7 커널, 2 스트라이드, 3 패딩): \n",
    "(28+2x3-7)/2+1=15\n",
    "(28+2x3-7)/2+1=15 -> 15x15x64\n",
    "\n",
    "첫 번째 MaxPool2d (3x3 커널, 2 스트라이드, 1 패딩): \n",
    "(15+2x1-3)/2+1=8\n",
    "(15+2x1-3)/2+1=8 -> 8x8x64\n",
    "\n",
    "이후의 계층들은 ResNet의 기본 블록\n",
    "이 블록들은 입력 특징 맵의 크기를 변경하지 않지만, 채널 수는 블록의 각 반복마다 두 배씩 증가할 수 있음.\n",
    "\n",
    "Layer1 (ResNet 블록 x2): 8x8x64\n",
    "Layer2 (ResNet 블록 x2, 첫 번째 블록에서 스트라이드=2로 다운샘플링): 4x4x128\n",
    "Layer3 (ResNet 블록 x2, 첫 번째 블록에서 스트라이드=2로 다운샘플링): 2x2x256\n",
    "Layer4 (ResNet 블록 x2, 첫 번째 블록에서 스트라이드=2로 다운샘플링): 1x1x512\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Only-Source Domain Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1719 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensor.backward() got an unexpected keyword argument 'distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22476f6c64227d/home/kiwan/_nex1/docs_DomainAdaptation/DANN_v2/dataset_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net\u001b[39m.\u001b[39m_train_source()\n",
      "File \u001b[0;32m~/_nex1/docs_DomainAdaptation/DANN_v2/domain.py:65\u001b[0m, in \u001b[0;36mNetwork._train_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m cls_predicted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC(source_feature\u001b[39m.\u001b[39mview(source_feature\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m# cls_predicted : [32, 10]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m classification_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCELoss(cls_predicted, source_labels)\n\u001b[0;32m---> 65\u001b[0m classification_loss\u001b[39m.\u001b[39mbackward(distance\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m bar\u001b[39m.\u001b[39mset_description(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.0f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m)]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mClass Loss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, params[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m], i \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(source_data), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_source), \u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m i \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_source), classification_loss\u001b[39m.\u001b[39mitem()))    \n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor.backward() got an unexpected keyword argument 'distance'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Define the transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 10),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
